{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第四次作业"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# 棋盘类\n",
    "class Env(object):\n",
    "    def __init__(self, row, column):\n",
    "        self.rewards = np.full((row, column), -1.0)\n",
    "        self.states = np.ones((row, column), dtype=np.int)  # 若有障碍物可设置状态为-1\n",
    "\n",
    "        self.index_list = [index for index, x in np.ndenumerate(self.states)]  # 可到达位置的索引号\n",
    "        self.ns_table = dict()  # 下一步可能移动到的位置\n",
    "        self._init_next_state_table()\n",
    "\n",
    "        self.rewards[0, 0] = 0\n",
    "        self.rewards[row - 1, column - 1] = 0\n",
    "\n",
    "        self.terminal = [(0, 0), (row - 1, column - 1)]\n",
    "\n",
    "    # 每个位置下一步可能到达位置的初始化\n",
    "    def _init_next_state_table(self):\n",
    "        for i, j in self.index_list:\n",
    "            next_states = list()\n",
    "            if (i - 1, j) in self.index_list:\n",
    "                next_states.append((i - 1, j))\n",
    "            else:\n",
    "                next_states.append((i, j))\n",
    "            if (i + 1, j) in self.index_list:\n",
    "                next_states.append((i + 1, j))\n",
    "            else:\n",
    "                next_states.append((i, j))\n",
    "            if (i, j - 1) in self.index_list:\n",
    "                next_states.append((i, j - 1))\n",
    "            else:\n",
    "                next_states.append((i, j))\n",
    "            if (i, j + 1) in self.index_list:\n",
    "                next_states.append((i, j + 1))\n",
    "            else:\n",
    "                next_states.append((i, j))\n",
    "            self.ns_table[(i, j)] = next_states\n",
    "\n",
    "    def get_reward(self, i, j):\n",
    "        return self.rewards[i, j]\n",
    "\n",
    "    def get_states(self):\n",
    "        return self.states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  物体类\n",
    "class Robot(object):\n",
    "    def __init__(self, env, gamma):\n",
    "        self._env = env\n",
    "        self._gamma = gamma  # 衰减因子\n",
    "        self.values = np.zeros_like(env.get_states(), dtype=np.float32)\n",
    "\n",
    "    def best_value_func(self, i, j):\n",
    "        if (i, j) in self._env.terminal:\n",
    "            return self._env.get_reward(i, j)\n",
    "        else:\n",
    "            return self._env.get_reward(i, j) + self._gamma * np.sum(self.next_states_expected_value(i, j))\n",
    "\n",
    "    def update_values(self):\n",
    "        next_values = np.zeros_like(self.values, dtype=np.float32)\n",
    "        for i, j in self._env.index_list:\n",
    "            next_values[i, j] = self.best_value_func(i, j)\n",
    "        self.values = next_values\n",
    "\n",
    "    def next_states_expected_value(self, i, j):\n",
    "        next_values = []\n",
    "        ps = [0.25, 0.25, 0.25, 0.25]\n",
    "        values = [self.values[i, j] for i, j in self._env.ns_table[(i, j)]]\n",
    "        next_values.append(np.multiply(values, ps))\n",
    "        return next_values\n",
    "\n",
    "    #  动作决策\n",
    "    def best_policy(self, i, j):\n",
    "        if (i, j) not in self._env.terminal:\n",
    "            print(\"(%d, %d)\" % (i, j), end=\" -> \")\n",
    "            next_states = self._env.ns_table[(i, j)]\n",
    "            best_state_index = np.argmax(self.next_states_expected_value(i, j))\n",
    "            best_state = next_states[best_state_index]\n",
    "            return self.best_policy(best_state[0], best_state[1])\n",
    "        print(\"(%d, %d)\" % (i, j))\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.       -13.999758 -19.999641 -21.999598]\n",
      " [-13.999758 -17.999683 -19.999641 -19.999641]\n",
      " [-19.999641 -19.999641 -17.999683 -13.999758]\n",
      " [-21.999598 -19.999641 -13.999758   0.      ]]\n"
     ]
    }
   ],
   "source": [
    "gamma = 1\n",
    "env = Env(4, 4)\n",
    "robot = Robot(env, gamma)\n",
    "\n",
    "for _ in range(200):\n",
    "    robot.update_values()\n",
    "print(robot.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2) -> (2, 2) -> (3, 2) -> (3, 3)\n"
     ]
    }
   ],
   "source": [
    "robot.best_policy(1, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
